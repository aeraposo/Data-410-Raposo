**Weak vs. Strong Learners**<br/>
	The efficacy of regression models is often evaluated based on their stance in the spectrum of learning strength. This learning strength is determined by the distribution of a model’s residuals. A residual is the distance between a model prediction and the true value of the dependent datapoint. Weak learners are characterized by residuals that are not normally distributed. Namely, parametric models fit on data following a non-linear trend, such as ![Math](https://render.githubusercontent.com/render/math?math=sin(x)), have residual distribution that mirror the distribution of the data. For example, below we see a linear model fit to ![Math](https://render.githubusercontent.com/render/math?math=sin(x)) next to a plot of model residuals, which are non-normally distributed and follow a similar trend to the datapoints themselves.<br/><br/>
<img src="./linear_residuals.png" width="600"><br/>
```
# the code used to produce the above plots
# generate some fake data
x = np.array(np.arange(0,5,0.1)).reshape((-1, 1))
y = np.array(np.sin(np.arange(0,5,0.1)).reshape((-1, 1)))
# fit a linear model
model.fit(x, y)
# make predictions using the model and calculate residuals
y_pred = model.predict(x)
resids = y-y_pred
# plot the results
plt.scatter(x,y, color = 'grey')
plt.plot(x,y_pred, color = 'k')
plt.scatter(x, resids, color = 'red', alpha = 0.2)
plt.title('Model predictions and residuals')
plt.legend(['Model','Data','Residuals'])
plt.hist(resids, color = 'grey', ec = 'black')
plt.title('Residual frequency distribution')
```
Unlike weak learners, strong learners have normally distributed residuals, which allows such models to better encompass the full scope of data represented and are therefore more accurate predictors. The normality assumption is particularly important because it allows us to meaninfully access model accuracy. <br/>

**Extreme Gradient Boosting**<br/>
	Boosting is a process in which algorithms convert weak learning models to stronger learners through transformations that improve the normalization of model residuals. Boosting is an important technique because in many cases there is not a well-performing or suitable model to choose from for the intended task at hand. For example, Neural Networks (NNs) are predominantly used in classification problems. Although a NN may be the optimal choice in this case, its performance may be hindered by certain qualities of the data such as insufficient feature representation or unexplained noise. In this case, boosting may be used to improve the accuracy of the model without compromising the use of this preferred modeling technique.
	One such method of boosting is called Extreme Gradient Boost, or XGBoost, is a decision tree based regression algorithm. Before unpacking the functionality and utility of XGBoost, we must first understand the inner working of its predecessor, Gradient Boosting. Suppose we have a dataset with n observations. The set’s independent variables are contained in a matrix *X* and the dependent variable y is contained in a column vector *y*. We know that the expected value of any single variable is simply its mean value so ![Math](https://render.githubusercontent.com/render/math?math=E(y)=\frac{sum y_i }{n}). The true values y_i observed in the dataset are each some distance away from the expected value of y. That is, ![Math](https://render.githubusercontent.com/render/math?math=y_i=E(y)+r_i) for ![Math](https://render.githubusercontent.com/render/math?math=i\in\{1,\dots,n\}). This distance (![Math](https://render.githubusercontent.com/render/math?math=r_i)) is known as the residual of a datapoint. One important assumption that we must abide by in linear and multinomial regression is the normality of residuals since the probability distribution of the beta parameters (model coefficients) are dependent on the distribution of the “error term” (![Math](https://render.githubusercontent.com/render/math?math=\epsilon)), ie, the residuals. Therefore, models made with this assumption where the residuals are not truly normally distributed will not encompass the complete trend within the dataset. Residuals should be checked for normality in smaller datasets but normality may be assumed for sufficiently large datasets since the central limit theorem holds.<br/>
Unlike regression algorithms such as Random Forest, the decision trees of Gradient Boost are not constructed based on the observed values of *X* and/or *y*, but rather, they are constructed based on the value of the residuals of each datapoint. After calculating the residuals ![Math](https://render.githubusercontent.com/render/math?math=r_i=y_i-E(y)), a forest of decision trees is then constructed. Each tree is constructed based on a random sample of the calculated residuals of the training dataset. Unlike in Random Forest, XGBoost trees predict residuals (residual = observed *y* value - predicted *y* value) and, as such, the hyper-parameter defined on each node is a residual. When constructing trees, terminal nodes represent the average value of the residuals of all datapoints that traverse to that node during training. Nodes are split on parameters that are determined useful through evaluation by trial and error with the gain function (as described in RFR tree construction in project 2). After training, these trees provide the predicted residual of the target variable from ![Math](https://render.githubusercontent.com/render/math?math=E(y)), the mean of y if no model is provided. The residuals of all trees are averaged to determine a final predicted residual of a testing datapoint, which is added to the mean (![Math](https://render.githubusercontent.com/render/math?math=E(y))) so attain a prediction for y for the test datapoint. That is,<br/>
![Math](https://render.githubusercontent.com/render/math?math=\hat{y}=E(y)+\hat{r})<br/>
An additional model parameter, which helps reduce overfitting is learning rate. Learning rate is a constant by which all residuals are multiplied when predictions are made and impacts the number of trees created, which allows for greater specificity in trees and lowers variance of predictions (and therefore higher accuracy)
![Math](https://render.githubusercontent.com/render/math?math=\hat{y}=f(x)=E(y))*+* learning rate![Math](https://render.githubusercontent.com/render/math?math=\cdot)residual
Overall, boosting helps normalize residuals by increasing the specificity of the model (reduces the distance between observed values and the model itself). This process, however, can lead to overfitting. XGBoost improves upon gradient boosting by reducing the likelihood of overfitting using several additional parameters.

During tree construction for XGBoosting, gain and loss functions are used to access the accuracy of a tree if a node split were to occur. Nodes are split if a positive gain and a sufficient decrease in loss results from a split. The sufficiency of a reduction in loss is defined by ![Math](https://render.githubusercontent.com/render/math?math=\Gamma) and ![Math](https://render.githubusercontent.com/render/math?math=\lambda). ![Math](https://render.githubusercontent.com/render/math?math=\Gamma) is “the minimum loss reduction required to make a further partition on a leaf node of the tree”. We hope to pick a threshold gamma that results in the highest possible gain. Additionally, ![Math](https://render.githubusercontent.com/render/math?math=\lambda) is a regularization parameter used in XGBoosting. ![Math](https://render.githubusercontent.com/render/math?math=\lambda) “reduces the prediction’s sensitivity to individual observation”, effectively balancing the generality and specificity of the provided model. Both parameters, ![Math](https://render.githubusercontent.com/render/math?math=\Gamma) and ![Math](https://render.githubusercontent.com/render/math?math=\lambda), are used to calculate gain and loss. This splitting evaluation process is repeated for each leaf, which splits between neighboring datapoints. The goal of splitting nodes is to maximize the collective gain of a tree while minimizing loss.
